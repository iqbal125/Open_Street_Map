{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OSM Data Wrangling </h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>\n",
    "\n",
    "In this project we will go over the data from the Open Street Map.\n",
    "\n",
    "<h4>Table of Contents: </h4>\n",
    "\n",
    "<a href=\"#a\">1. File and Sample Size</a>\n",
    "\n",
    "<a href=\"#b\"> 2. Counting Users and Tags </a>\n",
    "\n",
    "<a href=\"#c\"> 3. Auditing Ways and Tags </a>\n",
    "\n",
    "<a href=\"#d\"> 4. Auditing and Fixing Street Names </a>\n",
    "\n",
    "<a href=\"#e\"> 5. Auditing and Fixing ZIP Codes </a>\n",
    "\n",
    "<a href=\"#f\"> 6. Turning XML to CSV </a>\n",
    "\n",
    "<a href=\"#g\"> 7. SQL Queries </a>\n",
    "\n",
    "<a href=\"#h\"> 8. Conclusion </a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here we are importing our dependencies\n",
    "import xml.etree.ElementTree as ET\n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import codecs\n",
    "#import cerberus\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "filename = 'san-francisco_california.osm'\n",
    "\n",
    "OSM_PATH = 'san-francisco_california.osm'\n",
    "\n",
    "osm_file = 'san-francisco_california.osm'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"a\">File Size and Sample File</a></h3>\n",
    "\n",
    "This file is almost 1.4 gigabytes. This is far too large for us to work with effectively. Working with a dataset this large will most likely crash the internet browser. We will create a smaller file by iterively going over the tags and adding them to the sf_sample.osm file. We can check our code on the sample file and run our code on the entire dataset when we are finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397427031\n"
     ]
    }
   ],
   "source": [
    "size = os.path.getsize(filename)\n",
    "\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_file = \"sf_sample.osm\"\n",
    "\n",
    "k = 50\n",
    "\n",
    "#Here we are iterating over the enitering dataset and looking for \"start\" and \"end\" events\n",
    "def get_element(osm_file, tags=('node', 'way')):\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end':\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(sample_file, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write(b'<osm>\\n  ')\n",
    "\n",
    "    for i, element in enumerate(get_element(osm_file)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(b'</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"b\">Counting Users and Tags</a></h3>\n",
    "\n",
    "First we go over the tags and see the different types and number of tags. There are far more low-level tags than high level tags which is good because that is what we were expecting. Next, we will look at how many people have contributed to the open street map project. We see that 1764 different people have contributed to the project. This is a surprisingly small number in my opinion because as you can see in the next cell we see that the total number of tags is a little over 17 million, this means that the average user contributed almost 10,000 tags! Finally, in the last cell we look at the sample file and calculate that it has 3% of the tags of our original file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 54937,\n",
      " 'nd': 7774045,\n",
      " 'node': 6559055,\n",
      " 'osm': 1,\n",
      " 'relation': 6223,\n",
      " 'tag': 2123923,\n",
      " 'way': 807494}\n"
     ]
    }
   ],
   "source": [
    "#This code gives us a high-level overview of our file. We create a dictionary that will hold the tags, eays and nodes\n",
    "#and count the parent and child elements for each\n",
    "\n",
    "tags_dict = {}\n",
    "\n",
    "def count_tags_total(filename):\n",
    "    for elem in get_element(filename):\n",
    "        if elem.tag in tags_dict:\n",
    "            tags_dict[elem.tag] += 1\n",
    "        elif elem.tag not in tags_dict:\n",
    "            tags_dict[elem.tag] = 1\n",
    "             \n",
    "    return tags_dict\n",
    "\n",
    "counted_tags_total = count_tags_total(filename)\n",
    "\n",
    "pprint(counted_tags_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n"
     ]
    }
   ],
   "source": [
    "def count_users(filename):\n",
    "    unique_users = {}\n",
    "    for elem in get_element(filename):\n",
    "        for tag in elem.iter('way'):\n",
    "            if tag.attrib['user'] not in unique_users:\n",
    "                unique_users[tag.attrib['user']] = 1\n",
    "            else:\n",
    "                unique_users[tag.attrib['user']] += 1\n",
    "    \n",
    "    num_unique_users = len(unique_users)\n",
    "    print (num_unique_users)\n",
    "    \n",
    "    return unique_users\n",
    "\n",
    "count_users(filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17325677\n",
      "9821.812358276644\n"
     ]
    }
   ],
   "source": [
    "total_number_of_tags = tags_dict['nd'] + tags_dict['member'] + tags_dict['node'] + tags_dict['way'] + tags_dict['tag'] + tags_dict['relation']\n",
    "\n",
    "print(total_number_of_tags)\n",
    "\n",
    "total_users = 1764\n",
    "\n",
    "average_tags_per_user = total_number_of_tags/total_users\n",
    "\n",
    "print(average_tags_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 1825,\n",
      " 'nd': 311466,\n",
      " 'node': 131127,\n",
      " 'osm': 1,\n",
      " 'relation': 119,\n",
      " 'tag': 85163,\n",
      " 'way': 16286}\n",
      "545986\n",
      "0.03151311201288123\n"
     ]
    }
   ],
   "source": [
    "tags_dict_sample = {}\n",
    "\n",
    "def count_tags_sample(filename):\n",
    "    for elem in get_element(filename):\n",
    "        if elem.tag in tags_dict_sample:\n",
    "            tags_dict_sample[elem.tag] += 1\n",
    "        elif elem.tag not in tags_dict:\n",
    "            tags_dict_sample[elem.tag] = 1\n",
    "             \n",
    "    return tags_dict_sample\n",
    "\n",
    "counted_tags_sample = count_tags_sample(sample_file)\n",
    "\n",
    "total_number_of_tags_sample = tags_dict_sample['nd'] + tags_dict_sample['member'] + tags_dict_sample['node'] + tags_dict_sample['way'] + tags_dict_sample['tag'] + tags_dict_sample['relation']\n",
    "\n",
    "\n",
    "pprint(counted_tags_sample)\n",
    "\n",
    "print(total_number_of_tags_sample)\n",
    "\n",
    "percent_file_size = total_number_of_tags_sample/total_number_of_tags\n",
    "\n",
    "print(percent_file_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"c\">Auditing Ways and Tags</a></h3>\n",
    "\n",
    "We are looking closer at the ways tags here. This gives us a very basic overview of our ways tags. Going a little deeper in the next cell, we can clearly see familiar text such as street names and house numbers. Lets keep going further in our auditing process. There is a lot of information here stored in dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element 'way' at 0x263a278>,\n",
      " {'changeset': '36649660',\n",
      "  'id': '4970189',\n",
      "  'timestamp': '2016-01-18T06:57:39Z',\n",
      "  'uid': '296869',\n",
      "  'user': 'Zian Choy',\n",
      "  'version': '28'},\n",
      " <Element 'way' at 0x5c04390>,\n",
      " {'changeset': '5377692',\n",
      "  'id': '5149040',\n",
      "  'timestamp': '2010-08-02T07:27:12Z',\n",
      "  'uid': '24452',\n",
      "  'user': 'Speight',\n",
      "  'version': '4'},\n",
      " <Element 'way' at 0x5c046d8>,\n",
      " {'changeset': '41210984',\n",
      "  'id': '5149917',\n",
      "  'timestamp': '2016-08-03T10:09:20Z',\n",
      "  'uid': '933797',\n",
      "  'user': 'oba510',\n",
      "  'version': '8'},\n",
      " <Element 'way' at 0x5a2ca20>,\n",
      " {'changeset': '16977801',\n",
      "  'id': '6319820',\n",
      "  'timestamp': '2013-07-16T17:52:08Z',\n",
      "  'uid': '153669',\n",
      "  'user': 'dchiles',\n",
      "  'version': '9'},\n",
      " <Element 'way' at 0x5a7b048>,\n",
      " {'changeset': '445579',\n",
      "  'id': '6323786',\n",
      "  'timestamp': '2007-09-17T03:40:46Z',\n",
      "  'uid': '7168',\n",
      "  'user': 'DaveHansenTiger',\n",
      "  'version': '1'}]\n"
     ]
    }
   ],
   "source": [
    "#A counter of 5 is used to keep the code clean and concise\n",
    "def audit_ways(filename):\n",
    "    counter = 0\n",
    "    ways = []\n",
    "    for elem in get_element(filename):\n",
    "        if counter < 5:\n",
    "            for tag in elem.iter('way'):\n",
    "                    attributes = {\n",
    "                        \"id\": tag.attrib['id'],\n",
    "                        \"version\": tag.attrib['version'],\n",
    "                        'timestamp': tag.attrib['timestamp'],\n",
    "                        'changeset': tag.attrib['changeset'],\n",
    "                        'uid': tag.attrib['uid'],\n",
    "                        'user': tag.attrib['user']\n",
    "                    }\n",
    "                    ways.append(tag)\n",
    "                    ways.append(attributes)\n",
    "                    counter += 1\n",
    "    return ways\n",
    "\n",
    "sf_attributes = audit_ways(sample_file)\n",
    "\n",
    "pprint(sf_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "defaultdict(<type 'set'>, {'toilets': set(['yes']), 'fixme': set(['Needs Ground Survey. Road closed to through traffic near here']), 'exit_to': set(['Winton Avenue', '11th Street;14th Street']), 'barrier': set(['gate', 'lift_gate']), 'power': set(['tower']), 'name:zh': set([u'\\u65e7\\u91d1\\u5c71']), 'created_by': set(['JOSM']), 'source': set(['http://www.dot.ca.gov/hq/traffops/signtech/calnexus/pdf/980.pdf', 'http://www.dot.ca.gov/hq/traffops/signtech/calnexus/pdf/eighteightynorth.pdf;http://www.dot.ca.gov/hq/traffops/signtech/calnexus/pdf/eighteightysouth.pdf', 'PGS', 'local_knowledge']), 'ref:right': set(['17A']), 'foot': set(['yes']), 'crossing': set(['uncontrolled']), 'ref': set(['1A', '46', '25A', '27', '23']), 'exit_to:right': set(['2nd Street']), 'highway': set(['turning_circle', 'crossing', 'stop', 'traffic_signals', 'motorway_junction']), 'noref': set(['yes'])})\n"
     ]
    }
   ],
   "source": [
    "#We are now going a little bit deeper in our code and printing the attributes of each of the elements\n",
    "\n",
    "\n",
    "def audit_tags(filename):\n",
    "    counter = 0\n",
    "    attributes = defaultdict(set)\n",
    "    for elem in get_element(filename):\n",
    "        if counter < 100:\n",
    "            for tag in elem.iter('tag'):\n",
    "                attributes[tag.attrib['k']].add(tag.attrib['v'])\n",
    "                counter += 1\n",
    "\n",
    "    return attributes\n",
    "\n",
    "sf_attributes = audit_tags(sample_file)\n",
    "print(len(sf_attributes))\n",
    "\n",
    "pprint(sf_attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"d\"> Audit and Fixing Street Names</a> </h3> \n",
    "\n",
    "Here we are taking a deeper look at the data, more specifically the street names. We are auditing the street names and seeing if they match our expected spelling. You can see below that we have a several street names that are spelled incorrectly. This dataset is extremely large, so for now we will only focus on trying to fix the incorrect spelling of the street name suffix. From our audit of our sample file, we see that we have 3 examples of this issue: 'California Dr', 'Magnolia Ave' and 'Sawyer Camp Trail & Hillcrest Blvd'. We implement a function that loops through the incorrect names and removes the incorrect suffix, then appends the correct suffix to the street name. As you can see we successfully fixed these street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'addr:street': set(['Bridgeway', 'Sawyer Camp Trail & Hillcrest Blvd', 'The Alameda', 'Alameda', '12180142', 'Magnolia Ave', 'Rose Walk', 'Avenue H', 'Alameda De Las', 'California', 'Post', 'Garvin Avenie', 'Pacific Marina', 'San Clemente', 'El Camino Real', 'Wellesley Cres', 'Wildwood Gardens', 'Santa Cruz avenue', 'West El Camino Real', 'Woodside Plz', 'California Dr', 'Mission', 'San Francisco Bicycle Route 2', 'Broadway', 'Fort Mason'])})\n"
     ]
    }
   ],
   "source": [
    "expected = ['Street', 'Avenue', 'Drive', \n",
    "            'Road', 'Boulevard', 'Court',\n",
    "            'Place', 'Highway', 'Way', \n",
    "            'Alley', 'Plaza', 'Circle',\n",
    "            'Park', 'Lane', 'Parkway', \n",
    "            'Path', 'Road', 'Terrace',\n",
    "            'Center', 'Square', \"North\", \n",
    "            \"East\", \"West\", \"South\"]\n",
    "\n",
    "def audit_street_names(filename):\n",
    "    unexpected_street_names = defaultdict(set)\n",
    "    for elem in get_element(filename):\n",
    "        for tag in elem.iter('tag'):\n",
    "            if tag.attrib['k'] == 'addr:street':\n",
    "                #Declaring the street name and the suffix and adding it to the dictionary if it is not in the expected array\n",
    "                street_name = tag.attrib['v'].split()\n",
    "                street_name_suffix = street_name[-1]\n",
    "                for word in street_name:\n",
    "                     if street_name_suffix not in expected:\n",
    "                        unexpected_street_names[tag.attrib['k']].add(tag.attrib['v'])\n",
    "                        \n",
    "    return unexpected_street_names\n",
    "\n",
    "audited_street_names = audit_street_names(sample_file)\n",
    "\n",
    "pprint(audited_street_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['California Drive',\n",
      " 'Magnolia Avenue',\n",
      " 'Sawyer Camp Trail & Hillcrest Boulevard']\n"
     ]
    }
   ],
   "source": [
    "#This dictionary contains the incorrect abbreviated spelling as the key and the correct spelling as the value\n",
    "mapping = { \n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"STREET\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Ehs\": \"EHS\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Cir.\": \"Circle\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Ct.\": \"Court\",\n",
    "            \"Crt\": \"Court\",\n",
    "            \"Crt.\": \"Court\",\n",
    "            \"By-pass\": \"Bypass\",\n",
    "            \"N.\": \"North\",\n",
    "            \"N\": \"North\",\n",
    "            \"E.\": \"East\",\n",
    "            \"E\": \"East\",\n",
    "            \"S.\": \"South\",\n",
    "            \"S\": \"South\",\n",
    "            \"W.\": \"West\",\n",
    "            \"W\": \"West\"\n",
    "          }\n",
    "\n",
    "\n",
    "expected = ['Street', 'Avenue', 'Drive', \n",
    "            'Road', 'Boulevard', 'Court',\n",
    "            'Place', 'Highway', 'Way', \n",
    "            'Alley', 'Plaza', 'Circle',\n",
    "            'Park', 'Lane', 'Parkway', \n",
    "            'Path', 'Road', 'Terrace',\n",
    "            'Center', 'Square', \"North\", \n",
    "            \"East\", \"West\", \"South\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    In this fuction we remove the incorrect suffix \n",
    "    and append the correct spelling. Then we append the correct street name to the array.\n",
    "\"\"\"\n",
    "def fix_street_names(filename):\n",
    "    counter = 0\n",
    "    fixed_street_names_Arr = []\n",
    "    for elem in get_element(filename):\n",
    "        for tag in elem.iter('tag'):\n",
    "            if tag.attrib['k'] == 'addr:street':\n",
    "                street_name = tag.attrib['v'].split()\n",
    "                street_name_suffix = street_name[-1]\n",
    "                for word in street_name:\n",
    "                     if street_name_suffix not in expected and counter < 100:\n",
    "                        if street_name_suffix in mapping:\n",
    "                            correct_spelling = mapping.get(street_name_suffix)\n",
    "                            street_name_suffix = correct_spelling\n",
    "                            street_name.pop()\n",
    "                            street_name.append(street_name_suffix)\n",
    "                            fixed_street_names_Arr.append(street_name)\n",
    "                            counter += 1\n",
    "    \n",
    "    fixed_names = [' '.join(name) for name in fixed_street_names_Arr]\n",
    "    return fixed_names\n",
    "\n",
    "fixed_street_names = fix_street_names(sample_file)\n",
    "\n",
    "pprint(fixed_street_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id='e'>Auditing and Fixing ZIP codes</a></h3>\n",
    "\n",
    "We can also audit ZIP codes in a similar way that we audited the street names. In the next cell, we are running code to see if there are any ZIP Codes that dont match the expected 5 number format. We see that there are 5 examples of this. Lets fix that in the next cell. We can implement a simple solution to fix our ZIP codes. We simply loop over the incoorect formats and convert them to the 5 number format. As you can see the ZIP codes from the last cell are in the correct formatting now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94002-3585', 'CA 94544', '94301-2019', '94117-9991', '94118-1316']\n"
     ]
    }
   ],
   "source": [
    "def audit_zipcode(filename):\n",
    "    ways = []\n",
    "    for elem in get_element(filename):\n",
    "        if elem.tag == 'way':\n",
    "            for child in elem.iter('tag'):\n",
    "                if child.attrib['k'] == \"addr:postcode\":\n",
    "                    if len(child.attrib['v']) != 5:\n",
    "                        ways.append(child.attrib['v'])\n",
    "\n",
    "    return ways\n",
    "audited_zipcode = audit_zipcode(sample_file)\n",
    "\n",
    "pprint(audited_zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94002', '94544', '94301', '94117', '94118']\n"
     ]
    }
   ],
   "source": [
    "def fix_zip_codes(filename):\n",
    "    zip_codes = []\n",
    "    for elem in get_element(filename):\n",
    "        if elem.tag == 'way':\n",
    "            for child in elem.iter('tag'):\n",
    "                if child.attrib['k'] == \"addr:postcode\":\n",
    "                    zip_code = child.attrib['v']\n",
    "                    if len(zip_code) == 10:\n",
    "                        zip_code = zip_code[:-5]\n",
    "                        zip_codes.append(zip_code)\n",
    "                    elif len(zip_code) == 8:\n",
    "                        zip_code = zip_code[3:]\n",
    "                        zip_codes.append(zip_code)\n",
    "                    elif len(zip_code) != 5:\n",
    "                        zip_code.append(zip_codes)\n",
    "    return zip_codes\n",
    "                        \n",
    "fixed_zip_code = fix_zip_codes(sample_file)\n",
    "\n",
    "pprint(fixed_zip_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a id=\"f\"> Turning XML to CSV</a></h3>\n",
    "\n",
    "Here we will start to convert our XML file to CSV to use with SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_element(elem):\n",
    "\n",
    "    ways = {\n",
    "        'way': {},\n",
    "        'way_nodes': [],\n",
    "        'way_tags': []\n",
    "    }\n",
    "    nodes = {\n",
    "        'node': {},\n",
    "        'node_tags': []\n",
    "    }\n",
    "    \n",
    "    if elem.tag == 'way':\n",
    "        way_attributes = {\n",
    "            'id': elem.attrib['id'],\n",
    "            'version': elem.attrib['version'],\n",
    "            'timestamp': elem.attrib['timestamp'],\n",
    "            'changeset': elem.attrib['changeset'],\n",
    "            'uid': elem.attrib['uid'],\n",
    "            'user': elem.attrib['user']\n",
    "                 }\n",
    "        ways['way'].update(way_attributes)\n",
    "\n",
    "\n",
    "        node_counter = 0\n",
    "        for child in elem.iter('nd'):\n",
    "            way_node_attributes = {\n",
    "                'id': elem.attrib['id'],\n",
    "                'node_id': child.attrib['ref'],\n",
    "                'position': node_counter\n",
    "                  }\n",
    "            ways['way_nodes'].append(way_node_attributes)\n",
    "            node_counter += 1\n",
    "                \n",
    "        for child in elem.iter('tag'):\n",
    "            cid = elem.attrib['id']\n",
    "            key = child.attrib['k']\n",
    "            if ':' in key:\n",
    "                pos = key.find(':')\n",
    "                key = key[pos+1:]\n",
    "                ktype = key[:pos]\n",
    "            else:\n",
    "                ktype = 'regular'\n",
    "\n",
    "            tag_attributes = {\n",
    "                'id': elem.attrib['id'],\n",
    "                \"key\": child.attrib['k'],\n",
    "                'type': ktype,\n",
    "                'value': child.attrib['v']\n",
    "                    }\n",
    "            ways['way_tags'].append(tag_attributes)\n",
    "\n",
    "    if elem.tag == 'node':\n",
    "        node_attributes =  {'id': elem.attrib['id'],\n",
    "                      'user': elem.attrib['user'],\n",
    "                      'uid': elem.attrib['uid'],\n",
    "                      'version': elem.attrib['version'],\n",
    "                      'lat': elem.attrib['lat'],\n",
    "                      'lon': elem.attrib['lon'],\n",
    "                      'timestamp': elem.attrib['timestamp'],\n",
    "                      'changeset': elem.attrib['changeset']\n",
    "                    }\n",
    "\n",
    "        nodes['node'].update(node_attributes)\n",
    "        for child in elem.iter('tag'):\n",
    "            cid = elem.attrib['id']\n",
    "            key = child.attrib['k']\n",
    "            if ':' in key:\n",
    "                pos = key.find(':')\n",
    "                key = key[pos+1:]\n",
    "                ktype = key[:pos]\n",
    "            else:\n",
    "                ktype = 'regular'\n",
    "\n",
    "\n",
    "            node_tag_attrib = {\n",
    "                    'id':cid,\n",
    "                    'key': key,\n",
    "                    'value': child.attrib['v'],\n",
    "                    'type': ktype\n",
    "                }\n",
    "            nodes['node_tags'].append(node_tag_attrib)\n",
    "\n",
    "\n",
    "    if elem.tag =='node':\n",
    "        return nodes\n",
    "    elif elem.tag == 'way':\n",
    "        return ways\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Helper Functions\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Main Function\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "        \n",
    "#         validator = cerberus.Validator()\n",
    "\n",
    "        for elem in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(elem)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if elem.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif elem.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "\n",
    "process_map(sample_file, validate=False);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCHEMA = \"\"\"CREATE TABLE nodes ( \n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In this cell we will create all our tables for use in SQL queries\n",
    "\n",
    "\n",
    "db_file = 'sf_osm.db'\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Nodes\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\n",
    "    \"\"\"CREATE TABLE nodes ( \n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('nodes.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'].decode(\"utf-8\"), i['lon'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        Nodes_Tags\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('nodes_tags.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'],i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"),i['type']) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        Ways\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('ways.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, uid) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\"\"\"\n",
    "        Ways_Nodes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('ways_nodes.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'].decode(\"utf-8\"), i['position'].decode(\"utf-8\")) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\"\"\"\n",
    "        Ways_Tags\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "with open('ways_tags.csv','r') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode('utf-8')) for i in dr]\n",
    "\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <a id='g'> SQL Queries </a></h3>\n",
    "\n",
    "We will begin with very basic SQL Queries, then go deeper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes_tags\n",
      "[(30372583, u'ref', u'1A', u'regular'),\n",
      " (30372583,\n",
      "  u'source',\n",
      "  u'http://www.dot.ca.gov/hq/traffops/signtech/calnexus/pdf/980.pdf',\n",
      "  u'regular'),\n",
      " (30372583, u'exit_to', u'11th Street;14th Street', u'regular'),\n",
      " (30372583, u'highway', u'motorway_junction', u'regular'),\n",
      " (31817516, u'source', u'PGS', u'regular'),\n",
      " (31827991, u'source', u'PGS', u'regular'),\n",
      " (31849484, u'source', u'PGS', u'regular'),\n",
      " (31857880, u'source', u'PGS', u'regular'),\n",
      " (35706322, u'created_by', u'JOSM', u'regular'),\n",
      " (35706444, u'created_by', u'JOSM', u'regular')]\n"
     ]
    }
   ],
   "source": [
    "#Here we see the first 10 rows from each of our columns\n",
    "cur.execute('SELECT * FROM nodes_tags LIMIT 10')\n",
    "all_rows = cur.fetchall()\n",
    "print('nodes_tags:')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways:\n",
      "[(4970189, u'Zian Choy', 296869, u'28', 36649660, None),\n",
      " (5149040, u'Speight', 24452, u'4', 5377692, None),\n",
      " (5149917, u'oba510', 933797, u'8', 41210984, None),\n",
      " (6319820, u'dchiles', 153669, u'9', 16977801, None),\n",
      " (6323786, u'DaveHansenTiger', 7168, u'1', 445579, None),\n",
      " (6325632, u'DanHomerick', 160138, u'2', 11085608, None),\n",
      " (6326935, u'balrog-kun', 20587, u'2', 4244079, None),\n",
      " (6326959, u'balrog-kun', 20587, u'5', 4244079, None),\n",
      " (6327093, u'balrog-kun', 20587, u'2', 4244079, None),\n",
      " (6327122, u'KindredCoda', 14293, u'10', 10996498, None)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM ways LIMIT 10')\n",
    "all_rows = cur.fetchall()\n",
    "print(\"ways:\")\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways_tags:\n",
      "[(4970189, 32926684, 0),\n",
      " (4970189, 272607326, 1),\n",
      " (4970189, 1635648542, 2),\n",
      " (4970189, 541332511, 3),\n",
      " (4970189, 257013067, 4),\n",
      " (4970189, 541332512, 5),\n",
      " (4970189, 541332513, 6),\n",
      " (4970189, 29891951, 7),\n",
      " (4970189, 1635648546, 8),\n",
      " (4970189, 750523210, 9)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM ways_nodes LIMIT 10')\n",
    "all_rows = cur.fetchall()\n",
    "print(\"ways_nodes:\")\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways_tags:\n",
      "[(4970189, u'NHS', u'yes', u'regular'),\n",
      " (4970189, u'hgv', u'designated', u'regular'),\n",
      " (4970189, u'ref', u'CA 92', u'regular'),\n",
      " (4970189, u'name', u'J Arthur Younger Freeway', u'regular'),\n",
      " (4970189, u'lanes', u'2', u'regular'),\n",
      " (4970189, u'oneway', u'yes', u'regular'),\n",
      " (4970189, u'bicycle', u'no', u'regular'),\n",
      " (4970189, u'highway', u'motorway', u'regular'),\n",
      " (4970189, u'maxspeed', u'55 mph', u'regular'),\n",
      " (4970189, u'source:maxspeed', u'sign', u'maxspe')]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM ways_tags LIMIT 10')\n",
    "all_rows = cur.fetchall()\n",
    "print('ways_tags:')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes:\n",
      "[(29891972,\n",
      "  37.6714631,\n",
      "  -122.3912181,\n",
      "  u'KindredCoda',\n",
      "  14293,\n",
      "  6,\n",
      "  15682910,\n",
      "  u'2013-04-10T20:56:34Z'),\n",
      " (30029500,\n",
      "  37.6890322,\n",
      "  -122.1078783,\n",
      "  u'bill42',\n",
      "  482916,\n",
      "  20,\n",
      "  10936318,\n",
      "  u'2012-03-10T20:18:41Z'),\n",
      " (30033745,\n",
      "  37.8562329,\n",
      "  -122.4950289,\n",
      "  u'Speight',\n",
      "  24452,\n",
      "  7,\n",
      "  74256,\n",
      "  u'2009-04-02T07:39:02Z'),\n",
      " (30362588,\n",
      "  37.8366965,\n",
      "  -122.2958607,\n",
      "  u'Speight',\n",
      "  24452,\n",
      "  8,\n",
      "  5487612,\n",
      "  u'2010-08-14T06:23:43Z'),\n",
      " (30364114,\n",
      "  37.8228807,\n",
      "  -122.3011238,\n",
      "  u'dannykath',\n",
      "  2226712,\n",
      "  5,\n",
      "  36818926,\n",
      "  u'2016-01-26T15:09:31Z')]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT * FROM nodes LIMIT 5')\n",
    "all_rows = cur.fetchall()\n",
    "print('nodes:')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(722, 192565555)]\n"
     ]
    }
   ],
   "source": [
    "#Here we see that the most nodes a way tag has is 722 nodes! \n",
    "cur.execute(\"\"\"SELECT position, id\n",
    "               FROM ways_nodes\n",
    "               ORDER BY position DESC\n",
    "               LIMIT 1\n",
    "               \"\"\")\n",
    "\n",
    "all_rows = cur.fetchall()\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'AndrewSnow', 16286)]\n"
     ]
    }
   ],
   "source": [
    "#Here we have the top contributer of the ways tags with 16286 tags\n",
    "cur.execute(\"\"\" SELECT user, COUNT(*) as num\n",
    "                FROM  ways\n",
    "                ORDER BY num DESC\n",
    "                LIMIT 10;\n",
    "               \"\"\")\n",
    "\n",
    "all_rows = cur.fetchall()\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'restaurant', 63),\n",
      " (u'bench', 25),\n",
      " (u'cafe', 22),\n",
      " (u'drinking_water', 14),\n",
      " (u'fast_food', 13),\n",
      " (u'toilets', 13),\n",
      " (u'post_box', 12),\n",
      " (u'place_of_worship', 11),\n",
      " (u'school', 11),\n",
      " (u'bank', 8)]\n"
     ]
    }
   ],
   "source": [
    "#Here we have the top 10 amenities\n",
    "cur.execute(\"\"\" SELECT value, COUNT(*) as num\n",
    "                FROM nodes_tags\n",
    "                WHERE key='amenity'\n",
    "                GROUP BY value\n",
    "                ORDER BY num DESC\n",
    "                LIMIT 10;\n",
    "               \"\"\")\n",
    "\n",
    "all_rows = cur.fetchall()\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Irving Street', 10),\n",
      " (u'Church Street', 8),\n",
      " (u'Divisadero Street', 8),\n",
      " (u'Martin Luther King Jr Way', 7),\n",
      " (u'Page Street', 7),\n",
      " (u'Woodside Road', 7),\n",
      " (u'9th Avenue', 5),\n",
      " (u'Broadway', 5),\n",
      " (u'Dolores Street', 5),\n",
      " (u'Judah Street', 5)]\n"
     ]
    }
   ],
   "source": [
    "#Here we see that Irving Street is the most documented street in the nodes_tags\n",
    "cur.execute(\"\"\" SELECT value, COUNT(*) as num\n",
    "                FROM nodes_tags\n",
    "                WHERE key='street'\n",
    "                GROUP BY value\n",
    "                ORDER BY num DESC\n",
    "                LIMIT 10;\n",
    "               \"\"\")\n",
    "\n",
    "all_rows = cur.fetchall()\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3><a id='h'>Conclusion</a></h3>\n",
    "\n",
    "One problem I encountered was when I was looking through the tags after running my audit_tags function. The problem was with dead URLs that were cited as sources for certain tags. I noticed the dead URLs were usually government websites such as:  http://www.dot.ca.gov/hq/traffops/trucks/truckmap/ and ftp2.census.gov/geo/tiger/TIGER2013/ROADS/tl_2013_06081_roads. Some government URLs did actually work however, such as http://waterdata.usgs.gov/nwis/. The other URLs that were from the private sector generally worked such as 'https://www.propertyshark.com/mason/ca/Contra-Costa-County/Maps'. Without legitimate sources, people can just make up incorrect or inaccurate information, so this is a real issue. One way to correct this would be to periodically check if the URLs are valid. There might be a way to accomplish this programatically, but that is beyond my skill level. Another way to solve this would be government websites working with private websites. Private websites that sell information about real estate market trends, such as the previously mentioned propertyshark.com, have a real incentive to have correct and up to date information. Private websites might be willing to work with government websites because it will be a \"win win\" situation. The private website will have access to the government websites map information and resources and the government website will have access to the private websites information and resources. Having detailed map information about metropolitan areas is difficult and time-comsuming. By combining and pooling their resources, government and private websites will both be better off and have 1 very detailed map, instead of 2 not-very detailed maps. However, There are a lot obstacles to this. Government and Private orginizations generally do not work together for obvious reasons. There are also financial and planning issues that have to be accounted for.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<h4>Resources:</h4>\n",
    "<hr>\n",
    "\n",
    "https://discussions.udacity.com/t/p3-openstreetmap-overview/172045/2<br>\n",
    "https://discussions.udacity.com/t/final-project-need-help-getting-started/181525/60<br>\n",
    "https://discussions.udacity.com/t/count-tags-function-implementation/264133/2<br>\n",
    "https://discussions.udacity.com/t/system-ram-goes-to-99-while-wrangling/182980/7<br>\n",
    "https://wiki.openstreetmap.org/wiki/Elements<br>\n",
    "http://effbot.org/zone/element.htm<br>\n",
    "https://stackoverflow.com/questions/15881497/retrieving-xml-attribute-values-using-python-iterparse<br>\n",
    "https://discussions.udacity.com/t/elements-and-attributes/267615<br>\n",
    "https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md<br>\n",
    "http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python<br>\n",
    "https://stackoverflow.com/questions/15181867/understanding-the-set-function<br>\n",
    "https://discussions.udacity.com/t/printing-tags-for-keys-and-values/273966/3<br>\n",
    "https://stackoverflow.com/questions/5518435/python-fastest-way-to-create-a-list-of-n-lists<br>\n",
    "https://discussions.udacity.com/t/creating-a-list-of-lists-of-ways-tags/274031<br>\n",
    "https://discussions.udacity.com/t/help-with-count-users-function/274167<br>\n",
    "https://discussions.udacity.com/t/help-with-audit-street-names-function/274268<br>\n",
    "https://discussions.udacity.com/t/feedback-on-audit-street-names-method/275561<br>\n",
    "https://discussions.udacity.com/t/action-plan-for-p3/231324<br>\n",
    "https://github.com/brownan/SQLite-tutorial/blob/master/SQLite_Tutorial.ipynb<br>\n",
    "https://discussions.udacity.com/t/filenotfounderror-errno-2-no-such-file-or-directory-nodes-tags-csv/287485/14<br>\n",
    "https://discussions.udacity.com/t/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-32-37-character-maps-to-undefined/288697<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
